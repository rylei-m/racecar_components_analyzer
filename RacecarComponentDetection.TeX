\documentclass[12pt]{article}

% --------------------------------------------------
% Packages
% --------------------------------------------------
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{titlesec}

\geometry{margin=1in}

% --------------------------------------------------
% Formatting
% --------------------------------------------------
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    columns=fullflexible
}

\title{Racecar Component Detection Using YOLOv8 \\
\large CS5600/6600 --- Final Project}
\author{Rylei Mindrum}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This project develops and evaluates a data-driven computer vision system for detecting racecar components using YOLOv8. The purpose of this project has two key points: (1) to demonstrate a full supervised-learning pipeline aligned with the data-centric and model-centric AI engineering principles taught in CS5600/6600, and (2) to support ongoing research in aerodynamic simulation by enabling automatic identification of vehicle components such as tires, bumpers, glass panels, and chassis regions. The final system integrates two publicly available datasets, a reproducible dataset-merging pipeline, custom training scripts, and a complete evaluation suite.
\end{abstract}

% ==================================================
\section{Introduction}
Modern supervised learning pipelines require both high-quality data and well-engineered model training. In this project, I design an end-to-end object detection system capable of identifying racecar components from RGB images. This system leverages the YOLOv8 framework, integrates multiple heterogeneous datasets, and produces a unified model that detects both whole vehicles and smaller component categories.

The resulting model is designed with the intention of not only satisfying the project requirements of 5600, but also being an asset to my aerodynamic simulation research project. Detected components should be able to be used to parameterize vehicle configurations.

% ==================================================
\section{Background and Related Work}
Object detection has been widely studied across autonomous driving, robotics, and transportation systems. YOLO-based architectures have become standard in real-time object detection due to their computational efficiency and high accuracy.

YOLOv8 (by Ultralytics) provides:
\begin{itemize}[itemsep=2pt]
    \item anchor-free detection,
    \item improved non-max suppression and loss functions,
    \item streamlined training and export workflows.
\end{itemize}

Prior work on vehicle component detection typically relies on automotive datasets such as Pascal3D+, CityScapes, or proprietary OEM data. In contrast, this project integrates two publicly available datasets from Roboflow Universe and introduces a reproducible merging pipeline to create a single unified component-detection dataset suitable for racecar-focused simulation.

% ==================================================
\section{Datasets}

\subsection{Racecars Dataset}
The Racecars dataset provides images of various racing vehicles from track, rally, and street environments. It includes bounding boxes for whole vehicles and coarse structural regions. The dataset exhibits substantial variation in:
\begin{itemize}[itemsep=2pt]
    \item car types (GT, open-wheel, rally, drift)
    \item lighting and weather conditions
    \item occlusions, motion blur, and background clutter
\end{itemize}
This dataset provides global vehicle-level context important for downstream simulation and for training a \texttt{racecar} class.

\subsection{Car Components Dataset}
The Car Components dataset contains detailed annotations for 15 fine-grained automotive elements, including:
\begin{itemize}[itemsep=2pt]
    \item headlights and taillights (left/right)
    \item side mirrors (left/right)
    \item doors (left/right)
    \item front and rear bumpers
    \item front and rear glass
    \item wheels and hood
\end{itemize}
These classes are show high similarity and small pixel footprints, making detection more challenging and representative of realistic simulation requirements.

Originally, I planned to hand-annotate component-level labels on top of racecar images. After annotating about 25 images, it became clear that this approach was too time-consuming for the scope of this course project and the scope of my sanity. The Car Components dataset provided exactly what I needed, so I switched to using it and focused my engineering effort on dataset combination and model evaluation. Sorry for not getting prior approval.

\subsection{Merged Dataset Construction}

The two datasets were harmonized using a custom Python pre-processing script (provided in the .zip file):
\begin{itemize}[itemsep=2pt]
    \item converted all annotations to normalized YOLO text format
    \item ensured consistent class naming and index ordering
    \item added an additional \texttt{racecar} class from the Racecars dataset
    \item split the merged data into \texttt{train/}, \texttt{val/}, and \texttt{test/} sets
    \item removed duplicate or corrupted images
    \item applied filename prefixes (\texttt{cc\_} and \texttt{rc\_}) to avoid collisions
\end{itemize}

The final merged dataset contains 16 classes (15 components + \texttt{racecar}). The validation set used for quantitative evaluation contains 356 images and 2{,}041 annotated instances. the train and test set sizes can be adjusted but remain fixed for all experiments in this report.

This merged dataset allows the model to simultaneously reason about global vehicle structure and fine-grained components.

% ==================================================
\section{Methodology}

This project follows a complete supervised object detection pipeline using a data-driven AI engineering workflow. The framework consists of four major stages:

\begin{enumerate}
    \item \textbf{Dataset aggregation and harmonization}: Two publicly available datasets were merged into a consistent YOLO-format dataset. Standardized Label names, directory structures, and annotation styles.

    \item \textbf{Model training and optimization}: YOLOv8n model trained on merged dataset using Ultralytics PyTorch implementation. Training included: data augmentations, batch normalization, and learning rate scheduling.

    \item \textbf{Performance evaluation}: Model performance was measured using mAP, precision, recall, F1 score, confidence curves, and confusion matrices. Additional qualitative evaluation was performed using inference on unseen images.

    \item \textbf{Integration with simulation tools}: Outputs from the model were formatted into structured component dictionaries suitable for ingestion by my aerodynamic vehicle simulation pipeline.
\end{enumerate}

This methodology emphasizes reproducibility, systematic evaluation, and engineering-focused model deployment rather than maximizing absolute model size or dataset scale.

\subsection{Model Architecture}

Usage of the YOLOv8n model (nano variant), which is optimized for fast training, moderate resource requirements, and real-time inference.

\subsection{Training Procedure}

Training performed using the Ultralytics YOLOv8 framework. The key steps are summarized below.

\subsubsection*{Initialization}
\begin{itemize}[itemsep=2pt]
    \item Base model: \texttt{yolov8n.pt} (pretrained on COCO)
    \item Input resolution: $640 \times 640$ pixels
    \item Optimizer: AdamW with decoupled weight decay
    \item Learning rate schedule: linear warmup followed by cosine decay
\end{itemize}

\subsubsection*{Data Augmentation}
To increase robustness and reduce overfitting, the following augmentations were applied:
\begin{itemize}[itemsep=2pt]
    \item random horizontal flipping,
    \item mosaic augmentation,
    \item random brightness and saturation shifts,
    \item affine transformations (scale, translation, rotation),
    \item HSV color jitter.
\end{itemize}

\subsubsection*{Training Command}
Training was launched using the following command:

\begin{verbatim}
yolo task=detect mode=train \
    model=yolov8n.pt \
    data=data/merged/merged_data.yaml \
    epochs=50 \
    imgsz=640 \
    batch=8 \
    project=runs/train \
    name=merged_racecar_components_yolov8n
\end{verbatim}

Training ran for 50 epochs with a batch size of 8. Across epochs, all three loss
terms (classification, bounding box regression, and distribution focal loss)
decreased smoothly. The model converged stably, achieving strong validation
performance without significant overfitting.

\subsection{Training Hyperparameters}

\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{l c}
\hline
\textbf{Hyperparameter} & \textbf{Value} \\
\hline
Model architecture & YOLOv8n \\
Pretrained weights & \texttt{yolov8n.pt} \\
Image size & $640 \times 640$ \\
Batch size & 8 \\
Epochs & 50 \\
Optimizer & AdamW \\
Initial learning rate & 0.001 \\
Learning rate schedule & Cosine decay \\
Weight decay & 0.0005 \\
Momentum & 0.937 \\
IoU loss & DFL + CIoU \\
Confidence threshold (optimal) & 0.263 \\
Non-max suppression IoU & 0.7 \\
Augmentation strength & 0.5 \\
\hline
\end{tabular}
\caption{Hyperparameters used during YOLOv8n training.}
\label{tab:hyperparams}
\end{table}

% ==================================================
\section{Evaluation and Results}

Evaluation was performed using the held-out validation set consisting of 356
images and 2{,}041 annotated instances. Metrics include per-epoch loss curves,
precision, recall, F1 score, mean Average Precision (mAP), confidence curves,
and confusion matrices.

\subsection{Overall Detection Performance}

The trained detector achieves:
\begin{itemize}[itemsep=2pt]
    \item \textbf{mAP@50:} 0.7125,
    \item \textbf{mAP@50--95:} 0.5453,
    \item \textbf{Precision:} 0.674,
    \item \textbf{Recall:} 0.908.
\end{itemize}

Table~\ref{tab:summarymetrics} summarizes these metrics.

\begin{table}[H]
\centering
\begin{tabular}{l c}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Precision & 0.674 \\
Recall & 0.908 \\
mAP@50 & 0.7125 \\
mAP@50--95 & 0.5453 \\
\hline
\end{tabular}
\caption{Final YOLOv8n performance metrics on the validation set.}
\label{tab:summarymetrics}
\end{table}

These results indicate that the model is highly capable of identifying objects
with high recall while maintaining competitive precision. This trade-off is
favorable for downstream aerodynamic simulation where missing components is
more costly than occasionally detecting an extra one.

\subsection{Per-Class Average Precision}

Table~\ref{tab:ap_per_class} reports per-class AP scores from the evaluation
script.

\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{l c}
\hline
\textbf{Class} & \textbf{AP (mAP@50--95)} \\
\hline
back\_glass & 0.814 \\
back\_left\_side\_light & 0.463 \\
back\_right\_side\_light & 0.272 \\
door & 0.747 \\
front\_bumper & 0.695 \\
front\_glass & 0.854 \\
front\_left\_side\_door & 0.559 \\
front\_left\_side\_light & 0.410 \\
front\_left\_side\_mirror & 0.367 \\
front\_right\_side\_door & 0.357 \\
front\_right\_side\_light & 0.321 \\
front\_right\_side\_mirror & 0.309 \\
hood & 0.545 \\
rear\_bumper & 0.706 \\
wheel & 0.761 \\
\hline
\end{tabular}
\caption{Per-class Average Precision for the YOLOv8n merged dataset model.}
\label{tab:ap_per_class}
\end{table}

High-performing classes (AP $> 0.70$) include wheels, doors, bumpers, and
glass---components with strong geometric cues and large spatial extent.
Lower-performing classes correspond to side lights and mirrors, which are both
smaller and less visually distinct.

\subsection{Training and Validation Trends}

Ultralytics exports a consolidated training summary plot (\texttt{results.png})
that includes loss terms (box, cls, DFL) and key validation metrics across epochs.
Figure~\ref{fig:ultra_results} shows these trends for the merged YOLOv8n run.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\linewidth]{merged1full/figures/results.png}
    \caption{Ultralytics training summary plot (\texttt{results.png}) showing losses and validation metrics over 50 epochs.}
    \label{fig:ultra_results}
\end{figure}


\subsection{Confidence, Precision--Recall, and F1 Analysis}

Confidence-based evaluation curves provide insight into the optimal operating
point for the detector. The F1--confidence curve in
Figure~\ref{fig:f1_curve} indicates a maximum F1 score of approximately 0.76 at
a confidence threshold near $c = 0.263$. This threshold represents a good
balance between precision and recall and is used as the default for deployment.

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{merged1full/figures/BoxF1_curve.png}
    \caption{F1--Confidence curve. The optimal global threshold occurs near
    $c = 0.263$ with an F1 of approximately 0.76.}
    \label{fig:f1_curve}
\end{figure}

Precision--confidence and recall--confidence curves
(Figures~\ref{fig:precision_conf_curve} and~\ref{fig:recall_conf_curve}) show
that precision approaches 1.0 at high confidence thresholds, while recall
remains high for lower thresholds.

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{merged1full/figures/BoxP_curve.png}
    \caption{Precision--Confidence curve for all classes.}
    \label{fig:precision_conf_curve}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{merged1full/figures/BoxR_curve.png}
    \caption{Recall--Confidence curve for all classes.}
    \label{fig:recall_conf_curve}
\end{figure}

The precision--recall curve in Figure~\ref{fig:pr_curve} summarizes performance
across thresholds and yields the overall mAP values reported above.

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{merged1full/figures/BoxPR_curve.png}
    \caption{Precision--Recall curve for all classes. The area under these curves corresponds to the reported mAP scores.}
    \label{fig:pr_curve}
\end{figure}

\subsection{Confusion Matrix Analysis}

The confusion matrices in Figures~\ref{fig:confusion_matrix}
and~\ref{fig:confusion_matrix_norm} reveal the inter-class error structure.

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{merged1full/figures/confusion_matrix.png}
    \caption{Raw confusion matrix showing prediction frequencies across all classes.}
    \label{fig:confusion_matrix}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{merged1full/figures/confusion_matrix_normalized.png}
    \caption{Normalized confusion matrix showing per-class accuracy. High confusion is observed among fine-grained light and mirror classes.}
    \label{fig:confusion_matrix_norm}
\end{figure}

Normalized confusion matrix analysis shows strong diagonal dominance for most
classes, indicating high true positive rates. The largest confusions occur
within semantically similar groups, such as:
\begin{itemize}[itemsep=2pt]
    \item left vs.\ right side lights,
    \item left vs.\ right side mirrors,
    \item front vs.\ rear variants of door-like structures.
\end{itemize}
Cross-group confusions (ex: a mirror classified as a wheel) are extremely
rare, reflecting strong feature separation learned by the model.

\subsection{Qualitative Examples}

Qualitative examples generated using \texttt{visualize\_predictions.py} show that
predictions are well aligned with visible components even under challenging
lighting and partial occlusions. An example is shown in
Figure~\ref{fig:qualitative}.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{merged1full/figures/val_batch1_pred.jpg}
        \caption{Example 1.}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{merged1full/figures/val_batch2_pred.jpg}
        \caption{Example 2.}
    \end{subfigure}
    \caption{Qualitative validation detections produced by the merged YOLOv8n model.}
    \label{fig:qualitative}
\end{figure}


\subsection{Error Analysis}

Common failure modes include:
\begin{itemize}[itemsep=2pt]
    \item small or heavily occluded components (especially mirrors and side lights)
    \item overlapping parts where lights and door edges are visually similar
    \item motion blur in track images
    \item strong reflections on glass surfaces
    \item ambiguous annotation boundaries for elongated lights
\end{itemize}

% --- 2) Dataset label distribution + correlogram (helps justify failure modes/imbalance)
\subsection{Dataset Label Diagnostics}

Ultralytics generates label diagnostics that help explain class imbalance and
object-size difficulty. Figure~\ref{fig:labels_diagnostics} summarizes class
frequency and the relationship between label width/height.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.9\linewidth}
        \centering
        \includegraphics[width=\linewidth]{merged1full/figures/labels.jpg}
        \caption{Label distribution and box statistics (\texttt{labels.jpg}).}
        \label{fig:labels_jpg}
    \end{subfigure}
\end{figure}


% --- 3) Example training mosaic (helps show augmentations and data difficulty)
\subsection{Training Batch Visualization}

Figure~\ref{fig:train_batch} shows a representative training mosaic produced by
Ultralytics. This provides a qualitative view of augmentations (mosaic, scaling,
color jitter) and the diversity of component instances.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.90\linewidth]{merged1full/figures/train_batch0.jpg}
    \caption{Example augmented training batch mosaic (\texttt{train\_batch0.jpg}).}
    \label{fig:train_batch}
\end{figure}

% --- 5) (Optional) Put all curves into a single compact panel (if you want fewer pages)
\subsection{Compact Curve Panel}

If space is limited, Figure~\ref{fig:curve_panel} provides a compact view of the
core diagnostic curves in one place.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{merged1full/figures/BoxF1_curve.png}
        \caption{F1 vs confidence.}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{merged1full/figures/BoxPR_curve.png}
        \caption{Precision--Recall curve.}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{merged1full/figures/BoxP_curve.png}
        \caption{Precision vs confidence.}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{merged1full/figures/BoxR_curve.png}
        \caption{Recall vs confidence.}
    \end{subfigure}

    \caption{Compact summary of confidence and PR diagnostics.}
    \label{fig:curve_panel}
\end{figure}


% ==================================================
\section{Simulation Integration}

A major objective of this project is to connect vision-based component detection
with my aerodynamic simulation environment. To support this, the model's
predictions are transformed into structured Python objects that can be consumed
by the downstream simulation pipeline. This conversion is implemented in the
\texttt{to\_aero.py} module.

\subsection{Component Extraction Interface}

The module defines a lightweight \texttt{DetectedComponent} class encapsulating:
\begin{itemize}[itemsep=2pt]
    \item the component name predicted by the neural network
    \item the model's confidence score
    \item the pixel-space bounding box of the component
\end{itemize}
These abstractions allow the aerodynamic code to interact with components
independently of the underlying detection architecture. Below you will find the code from to\_aero.py.

\begin{lstlisting}[language=Python, caption={Interface for converting YOLO detections into simulation-ready component objects.}, label={lst:to_aero}]
from ultralytics import YOLO

class DetectedComponent:
    def __init__(self, cls_name, conf, x1, y1, x2, y2):
        self.cls_name = cls_name
        self.conf = conf
        self.bbox = (x1, y1, x2, y2)

    def __repr__(self):
        return f"{self.cls_name}({self.conf:.2f}): {self.bbox}"

def analyze_components(model_path, image_path):
    model = YOLO(model_path)
    results = model(image_path)[0]

    detected = []
    names = model.names

    for box in results.boxes:
        cls_id = int(box.cls)
        conf = float(box.conf)
        x1, y1, x2, y2 = box.xyxy[0].tolist()

        detected.append(
            DetectedComponent(
                cls_name=names[cls_id],
                conf=conf,
                x1=x1, y1=y1, x2=x2, y2=y2
            )
        )

    return detected
\end{lstlisting}

\subsection{Example Component Extraction Output}

When the trained YOLOv8n model is applied to a racecar frame, the interface
returns a list of detected components. A representative output is:

\begin{verbatim}
[
  front_bumper(0.94): (112, 240, 398, 360),
  front_glass(0.96): (140, 110, 420, 220),
  wheel(0.98): (75, 260, 155, 340),
  wheel(0.97): (360, 255, 445, 345),
  front_left_side_mirror(0.71): (130, 195, 160, 220)
]
\end{verbatim}

Each entry includes the detected class name, a confidence score, and bounding
box coordinates $(x_1, y_1, x_2, y_2)$ in pixel space. This compact
representation allows the aerodynamic system to query specific components
without needing to understand the detection modelâ€™s internal details.

\subsection{From Bounding Boxes to Aerodynamic Features}

Bounding boxes extracted from \texttt{analyze\_components()} are converted into
simulation-relative coordinates:
\[
x' = \frac{x - x_{\min}}{W}, \qquad
y' = \frac{y - y_{\min}}{H},
\]
where $W$ and $H$ denote the width and height of the image or rendered frame.
This normalization allows consistent integration with 2D or 3D vehicle models.

Common uses:
\begin{itemize}[itemsep=2pt]
    \item identifying aerodynamic surfaces such as bumpers, wings, and mirrors
    \item estimating projected frontal area
    \item adjusting drag or lift coefficients based on detected configuration
    \item tracking dynamic occlusions during vehicle motion
    \item generating mesh masks for downstream GPU-based CFD solvers
\end{itemize}

A convenience dictionary representation used by the simulation code is:
\begin{verbatim}
{
  "component": "front_left_side_mirror",
  "bbox": [x_center, y_center, width, height],
  "confidence": 0.87
}
\end{verbatim}
The resulting pipeline allows vision-based state estimation to drive downstream
automotive engineering analysis.

% ==================================================
\section{Discussion and Future Work}

The model performs strongly overall, with particularly high detection accuracy
for large, high-contrast components (wheels, glass, bumpers, doors). More
subtle components such as side lights and mirrors remain challenging due to:
\begin{itemize}[itemsep=2pt]
    \item small pixel footprint
    \item high intra-class visual similarity
    \item symmetric left/right geometry
    \item limited annotated samples per class
\end{itemize}

Despite these challenges, the YOLOv8n model exceeds expectations for a
lightweight architecture trained on merged heterogeneous datasets.

Future work includes:
\begin{itemize}[itemsep=2pt]
    \item converting bounding boxes to geometric primitives for use in CFD
    \item extending the system to video for temporal tracking of components
    \item exploring semantic segmentation for finer shape modeling
    \item performing domain adaptation to race-day camera feeds and new tracks
    \item experimenting with larger YOLOv8 variants (e.g., \texttt{yolov8s/m})
          to improve small-object detection
\end{itemize}

% ==================================================
\section{Conclusion}

This project demonstrates a complete AI engineering lifecycle: dataset
construction, model training, evaluation, error analysis, and integration with a
downstream mechanical engineering application. The merged YOLOv8n model
successfully detects both whole vehicles and component-level classes, providing
actionable outputs for aerodynamic simulation and research. The work highlights
how data-centric engineering and systematic evaluation can produce useful
real-world tools, even when using relatively small models and heterogeneous
public datasets. Thank you so much for a great class!

\newpage
\begin{thebibliography}{99}

\bibitem{car-components-dataset}
Sammy.
\textit{Car Components Dataset}.
Roboflow Universe, Open Source Dataset, August 2023.
Available at:
\url{https://universe.roboflow.com/sammy/car-components-dataset}.

\bibitem{racecars-n6toi}
Tyrone Brock.
\textit{Racecars Dataset}.
Roboflow Universe, Open Source Dataset.
Available at:
\url{https://universe.roboflow.com/tyrone-brock/racecars-n6toi}.

\bibitem{wiki-automotive-aerodynamics}
Wikipedia contributors.
\textit{Automotive aerodynamics}.
Wikipedia, The Free Encyclopedia.
Available at:
\url{https://en.wikipedia.org/wiki/Automotive_aerodynamics}.
Accessed November 19, 2025.

\bibitem{cfd-modelling-vehicle-aerodynamics}
CFD Flow Engineering.
\textit{CFD Modelling of Vehicle Aerodynamics}.
Available at:
\url{https://cfdflowengineering.com/cfd-modelling-of-vehicle-aerodynamics/}.
Accessed November 19, 2025.

\bibitem{science-article-vehicle-aerodynamics}
P. Qin et al.
\textit{CFD simulation of aerodynamic forces on the DrivAer car model}.
\textit{Journal of Wind Engineering and Industrial Aerodynamics}.
Available at:
\url{https://www.sciencedirect.com/science/article/pii/S0894177716303119}.
Accessed December 02, 2025.
\end{thebibliography}




\end{document}
